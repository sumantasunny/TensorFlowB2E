{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du1/13CS30041/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "from tensorflow.contrib import rnn\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.set_random_seed(123)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "(49500, 784)\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "Epoch : 0 Training Loss: 0.5921337350450379\n",
      "Validation Accuracy: 0.88618183\n",
      "Epoch : 1 Training Loss: 0.25245149212353146\n",
      "Validation Accuracy: 0.9367273\n",
      "Epoch : 2 Training Loss: 0.19688182293315137\n",
      "Validation Accuracy: 0.9383636\n",
      "Epoch : 3 Training Loss: 0.1628297660687957\n",
      "Validation Accuracy: 0.95436364\n",
      "Epoch : 4 Training Loss: 0.14439215468186317\n",
      "Validation Accuracy: 0.9541818\n",
      "Epoch : 5 Training Loss: 0.1326256215609986\n",
      "Validation Accuracy: 0.954\n",
      "Epoch : 6 Training Loss: 0.12180089174835652\n",
      "Validation Accuracy: 0.9647273\n",
      "Epoch : 7 Training Loss: 0.1150093343917921\n",
      "Validation Accuracy: 0.9670909\n",
      "Epoch : 8 Training Loss: 0.11105018466560526\n",
      "Validation Accuracy: 0.9670909\n",
      "Epoch : 9 Training Loss: 0.10595521776363101\n",
      "Validation Accuracy: 0.968\n",
      "Epoch : 10 Training Loss: 0.10021134389169285\n",
      "Validation Accuracy: 0.96854544\n",
      "Epoch : 11 Training Loss: 0.09432064285972207\n",
      "Validation Accuracy: 0.9663636\n",
      "Epoch : 12 Training Loss: 0.09603423253736555\n",
      "Validation Accuracy: 0.96509093\n",
      "Epoch : 13 Training Loss: 0.09168040585243195\n",
      "Validation Accuracy: 0.97254544\n",
      "Epoch : 14 Training Loss: 0.08216800927937093\n",
      "Validation Accuracy: 0.9670909\n",
      "Epoch : 15 Training Loss: 0.08902869258132388\n",
      "Validation Accuracy: 0.97\n",
      "Epoch : 16 Training Loss: 0.07966531771564424\n",
      "Validation Accuracy: 0.9670909\n",
      "INFO:tensorflow:global_step/sec: 71.4585\n",
      "Epoch : 17 Training Loss: 0.08457522522846253\n",
      "Validation Accuracy: 0.966\n",
      "Epoch : 18 Training Loss: 0.07550485017050927\n",
      "Validation Accuracy: 0.9752727\n",
      "Epoch : 19 Training Loss: 0.07504628168306117\n",
      "Validation Accuracy: 0.9736364\n",
      "Epoch : 20 Training Loss: 0.07080334272033383\n",
      "Validation Accuracy: 0.9678182\n",
      "Epoch : 21 Training Loss: 0.07482593402247693\n",
      "Validation Accuracy: 0.96927273\n",
      "Epoch : 22 Training Loss: 0.07164269649832876\n",
      "Validation Accuracy: 0.97345454\n",
      "Epoch : 23 Training Loss: 0.06733289643313085\n",
      "Validation Accuracy: 0.9661818\n",
      "Epoch : 24 Training Loss: 0.0677218125610979\n",
      "Validation Accuracy: 0.97327274\n",
      "Epoch : 25 Training Loss: 0.06520698905988313\n",
      "Validation Accuracy: 0.9672727\n",
      "Epoch : 26 Training Loss: 0.06270664052595616\n",
      "Validation Accuracy: 0.97345454\n",
      "Epoch : 27 Training Loss: 0.06583387593320114\n",
      "Validation Accuracy: 0.97145456\n",
      "Epoch : 28 Training Loss: 0.06277085779272398\n",
      "Validation Accuracy: 0.9729091\n",
      "Epoch : 29 Training Loss: 0.06624169488757319\n",
      "Validation Accuracy: 0.97163635\n",
      "Early stopping ...\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.eye(10)[np.asarray(mnist.train.labels, dtype=np.int32)]\n",
    "test_data = mnist.test.images  # Returns np.array\n",
    "test_labels = np.eye(10)[np.asarray(mnist.test.labels, dtype=np.int32)]\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=123)\n",
    "\n",
    "\n",
    "print train_data.shape\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "num_hidden = 100\n",
    "timesteps = 28\n",
    "num_classes=10\n",
    "\n",
    "class SimpleRNNCell(RNNCell):\n",
    "    def __init__(self, num_units, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "        self.matrix = []\n",
    "    \n",
    "    def _linear(self, args, output_size, bias, bias_start=0.0, scope=None):\n",
    "        \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
    "        Args:\n",
    "          args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
    "          output_size: int, second dimension of W[i].\n",
    "          bias: boolean, whether to add a bias term or not.\n",
    "          bias_start: starting value to initialize the bias; 0 by default.\n",
    "          scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "        Returns:\n",
    "          A 2D Tensor with shape [batch x output_size] equal to\n",
    "          sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "        Raises:\n",
    "          ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "        \"\"\"\n",
    "        if args is None or (isinstance(args, (list, tuple)) and not args):\n",
    "            raise ValueError(\"`args` must be specified\")\n",
    "        if not isinstance(args, (list, tuple)):\n",
    "            args = [args]\n",
    "\n",
    "        # Calculate the total size of arguments on dimension 1.\n",
    "        total_arg_size = 0\n",
    "        shapes = [a.get_shape().as_list() for a in args]\n",
    "        for shape in shapes:\n",
    "            if len(shape) != 2:\n",
    "                raise ValueError(\n",
    "                    \"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "            if not shape[1]:\n",
    "                raise ValueError(\n",
    "                    \"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "            else:\n",
    "                total_arg_size += shape[1]\n",
    "\n",
    "        # Now the computation.\n",
    "        with tf.variable_scope(scope or \"Linear\"):\n",
    "            self.matrix = tf.get_variable(\"Matrix\", [total_arg_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            if len(args) == 1:\n",
    "                res = tf.matmul(args[0], self.matrix)\n",
    "            else:\n",
    "                res = tf.matmul(tf.concat(args,1), self.matrix)\n",
    "            if not bias:\n",
    "                return res\n",
    "            bias_term = tf.get_variable(\n",
    "                \"Bias\", [output_size],\n",
    "                initializer=tf.constant_initializer(bias_start))\n",
    "        return res + bias_term\n",
    "\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n",
    "            output = self._activation(self._linear([inputs, state], self._num_units, True))\n",
    "        return output, output #h, state\n",
    "\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "            self.mode = tf.placeholder(tf.bool)\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "            self.input_layer = tf.reshape(self.X, [-1, 28, 28])\n",
    "            \n",
    "            self.w_out = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "            self.b_out = tf.Variable(tf.random_normal([num_classes]))\n",
    "            \n",
    "            \n",
    "            # Prepare data shape to match `rnn` function requirements\n",
    "            # Current data input shape: (batch_size, timesteps, n_input)\n",
    "            # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "            # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "            self.x = tf.unstack(self.input_layer, timesteps, 1)\n",
    "            rnn_cell = SimpleRNNCell(num_hidden)\n",
    "            outputs, states = rnn.static_rnn(rnn_cell, self.x, dtype=tf.float32)\n",
    "            self.logits = tf.matmul(outputs[-1], self.w_out) + self.b_out\n",
    "            self.matrix = rnn_cell.matrix\n",
    "\n",
    "            self.pred = tf.nn.softmax(self.logits)\n",
    "            # Test model and check accuracy\n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            # define cost/loss & optimizer\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            tf.summary.scalar('mean_loss', self.cost)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n",
    "            # When using the batchnormalization layers,\n",
    "            # it is necessary to manually add the update operations\n",
    "            # because the moving averages are not included in the graph            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):                     \n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost, global_step=self.global_step)\n",
    "\n",
    "nn = RNN()\n",
    "# Best validation accuracy seen so far.\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "# Iteration-number for last improvement to validation accuracy.\n",
    "last_improvement = 0\n",
    "\n",
    "# Stop optimization if no improvement found in this many iterations.\n",
    "patience = 10\n",
    "\n",
    "# Start session\n",
    "sv = tf.train.Supervisor(graph=nn.graph,\n",
    "                         logdir='logs_rnn/',\n",
    "                         summary_op=None,\n",
    "                         save_model_secs=0)\n",
    "\n",
    "with sv.managed_session(config=tf.ConfigProto(device_count={'GPU':1})) as sess:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(len(train_data) / batch_size)\n",
    "        if sv.should_stop(): break\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = train_data[(i)*batch_size:(i+1)*batch_size], train_labels[(i)*batch_size:(i+1)*batch_size]\n",
    "            feed_dict = {nn.X: batch_xs, nn.Y: batch_ys, nn.mode:True, nn.keep_prob:0.8}\n",
    "            c, _ = sess.run([nn.cost, nn.optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "            if i%50:\n",
    "                \n",
    "                sv.summary_computed(sess, sess.run(nn.merged, feed_dict))\n",
    "                gs = sess.run(nn.global_step, feed_dict)\n",
    "        \n",
    "        print 'Epoch : ' + str(epoch) + ' Training Loss: ' + str(avg_cost)\n",
    "        acc = sess.run(nn.accuracy, feed_dict={\n",
    "                        nn.X: val_data, nn.Y: val_labels, nn.mode:False, nn.keep_prob:1.0})\n",
    "        print 'Validation Accuracy: ' + str(acc)\n",
    "        if acc > best_validation_accuracy:\n",
    "            last_improvement = epoch\n",
    "            best_validation_accuracy = acc\n",
    "            sv.saver.save(sess, 'logs_rnn' + '/model_gs', global_step=gs)\n",
    "        if epoch - last_improvement > patience:\n",
    "            print(\"Early stopping ...\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded\n",
      "INFO:tensorflow:Starting standard services.\n",
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from logs_rnn/model_gs-9405\n",
      "Restored!\n",
      "('Accuracy:', 0.9715)\n"
     ]
    }
   ],
   "source": [
    "nn = RNN()\n",
    "print(\"Graph loaded\")\n",
    "with nn.graph.as_default():\n",
    "    sv = tf.train.Supervisor()\n",
    "    with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "        ## Restore parameters\n",
    "        sv.saver.restore(sess, tf.train.latest_checkpoint('logs_rnn/'))\n",
    "        print(\"Restored!\")\n",
    "        acc = sess.run(nn.accuracy, feed_dict={\n",
    "              nn.X: test_data, nn.Y: test_labels, nn.mode:False, nn.keep_prob:1.0})\n",
    "        print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
